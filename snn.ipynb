{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing  import StandardScaler, OneHotEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"input/train.csv\")\n",
    "test_data = pd.read_csv(\"input/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10881</th>\n",
       "      <td>2012-12-19 19:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.58</td>\n",
       "      <td>19.695</td>\n",
       "      <td>50</td>\n",
       "      <td>26.0027</td>\n",
       "      <td>7</td>\n",
       "      <td>329</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10882</th>\n",
       "      <td>2012-12-19 20:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.76</td>\n",
       "      <td>17.425</td>\n",
       "      <td>57</td>\n",
       "      <td>15.0013</td>\n",
       "      <td>10</td>\n",
       "      <td>231</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10883</th>\n",
       "      <td>2012-12-19 21:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.94</td>\n",
       "      <td>15.910</td>\n",
       "      <td>61</td>\n",
       "      <td>15.0013</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10884</th>\n",
       "      <td>2012-12-19 22:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.94</td>\n",
       "      <td>17.425</td>\n",
       "      <td>61</td>\n",
       "      <td>6.0032</td>\n",
       "      <td>12</td>\n",
       "      <td>117</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>2012-12-19 23:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.12</td>\n",
       "      <td>16.665</td>\n",
       "      <td>66</td>\n",
       "      <td>8.9981</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10886 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime  season  holiday  workingday  weather   temp  \\\n",
       "0      2011-01-01 00:00:00       1        0           0        1   9.84   \n",
       "1      2011-01-01 01:00:00       1        0           0        1   9.02   \n",
       "2      2011-01-01 02:00:00       1        0           0        1   9.02   \n",
       "3      2011-01-01 03:00:00       1        0           0        1   9.84   \n",
       "4      2011-01-01 04:00:00       1        0           0        1   9.84   \n",
       "...                    ...     ...      ...         ...      ...    ...   \n",
       "10881  2012-12-19 19:00:00       4        0           1        1  15.58   \n",
       "10882  2012-12-19 20:00:00       4        0           1        1  14.76   \n",
       "10883  2012-12-19 21:00:00       4        0           1        1  13.94   \n",
       "10884  2012-12-19 22:00:00       4        0           1        1  13.94   \n",
       "10885  2012-12-19 23:00:00       4        0           1        1  13.12   \n",
       "\n",
       "        atemp  humidity  windspeed  casual  registered  count  \n",
       "0      14.395        81     0.0000       3          13     16  \n",
       "1      13.635        80     0.0000       8          32     40  \n",
       "2      13.635        80     0.0000       5          27     32  \n",
       "3      14.395        75     0.0000       3          10     13  \n",
       "4      14.395        75     0.0000       0           1      1  \n",
       "...       ...       ...        ...     ...         ...    ...  \n",
       "10881  19.695        50    26.0027       7         329    336  \n",
       "10882  17.425        57    15.0013      10         231    241  \n",
       "10883  15.910        61    15.0013       4         164    168  \n",
       "10884  17.425        61     6.0032      12         117    129  \n",
       "10885  16.665        66     8.9981       4          84     88  \n",
       "\n",
       "[10886 rows x 12 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "break datetime to date, hour, weekday, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_datetime(data):\n",
    "    data['date'] = data.datetime.apply(lambda x: x.split()[0])\n",
    "    data['hour'] = data.datetime.apply(lambda x: x.split()[1].split(':')[0]).astype('int')\n",
    "    data['weekday'] = data.date.apply(lambda dateString: datetime.strptime(dateString, '%Y-%m-%d').weekday())\n",
    "    data['month'] = data.date.apply(lambda dateString: datetime.strptime(dateString, '%Y-%m-%d').month)\n",
    "    data = data.drop(['datetime', 'date'], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_datetime = break_datetime(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10881</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.58</td>\n",
       "      <td>19.695</td>\n",
       "      <td>50</td>\n",
       "      <td>26.0027</td>\n",
       "      <td>7</td>\n",
       "      <td>329</td>\n",
       "      <td>336</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10882</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.76</td>\n",
       "      <td>17.425</td>\n",
       "      <td>57</td>\n",
       "      <td>15.0013</td>\n",
       "      <td>10</td>\n",
       "      <td>231</td>\n",
       "      <td>241</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10883</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.94</td>\n",
       "      <td>15.910</td>\n",
       "      <td>61</td>\n",
       "      <td>15.0013</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "      <td>168</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10884</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.94</td>\n",
       "      <td>17.425</td>\n",
       "      <td>61</td>\n",
       "      <td>6.0032</td>\n",
       "      <td>12</td>\n",
       "      <td>117</td>\n",
       "      <td>129</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.12</td>\n",
       "      <td>16.665</td>\n",
       "      <td>66</td>\n",
       "      <td>8.9981</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>88</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10886 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       season  holiday  workingday  weather   temp   atemp  humidity  \\\n",
       "0           1        0           0        1   9.84  14.395        81   \n",
       "1           1        0           0        1   9.02  13.635        80   \n",
       "2           1        0           0        1   9.02  13.635        80   \n",
       "3           1        0           0        1   9.84  14.395        75   \n",
       "4           1        0           0        1   9.84  14.395        75   \n",
       "...       ...      ...         ...      ...    ...     ...       ...   \n",
       "10881       4        0           1        1  15.58  19.695        50   \n",
       "10882       4        0           1        1  14.76  17.425        57   \n",
       "10883       4        0           1        1  13.94  15.910        61   \n",
       "10884       4        0           1        1  13.94  17.425        61   \n",
       "10885       4        0           1        1  13.12  16.665        66   \n",
       "\n",
       "       windspeed  casual  registered  count  hour  weekday  month  \n",
       "0         0.0000       3          13     16     0        5      1  \n",
       "1         0.0000       8          32     40     1        5      1  \n",
       "2         0.0000       5          27     32     2        5      1  \n",
       "3         0.0000       3          10     13     3        5      1  \n",
       "4         0.0000       0           1      1     4        5      1  \n",
       "...          ...     ...         ...    ...   ...      ...    ...  \n",
       "10881    26.0027       7         329    336    19        2     12  \n",
       "10882    15.0013      10         231    241    20        2     12  \n",
       "10883    15.0013       4         164    168    21        2     12  \n",
       "10884     6.0032      12         117    129    22        2     12  \n",
       "10885     8.9981       4          84     88    23        2     12  \n",
       "\n",
       "[10886 rows x 14 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change weather and season to one-hot columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_transform(data):\n",
    "    season_one_hot = OneHotEncoder()\n",
    "    scol = season_one_hot.fit_transform(data['season'].values.reshape(-1, 1)).toarray()\n",
    "    scol = pd.DataFrame(scol)\n",
    "    data = pd.concat([data, scol], axis=1)\n",
    "    data.drop(['season'], axis=1)\n",
    "\n",
    "    weather_one_hot = OneHotEncoder()\n",
    "    wcol = weather_one_hot.fit_transform(data['weather'].values.reshape(-1, 1)).toarray()\n",
    "    wcol = pd.DataFrame(wcol)\n",
    "    data = pd.concat([data, wcol], axis=1)\n",
    "    data.drop(['weather'], axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_one_hot = one_hot_transform(data_no_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>...</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10881</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.58</td>\n",
       "      <td>19.695</td>\n",
       "      <td>50</td>\n",
       "      <td>26.0027</td>\n",
       "      <td>7</td>\n",
       "      <td>329</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10882</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.76</td>\n",
       "      <td>17.425</td>\n",
       "      <td>57</td>\n",
       "      <td>15.0013</td>\n",
       "      <td>10</td>\n",
       "      <td>231</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10883</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.94</td>\n",
       "      <td>15.910</td>\n",
       "      <td>61</td>\n",
       "      <td>15.0013</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10884</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.94</td>\n",
       "      <td>17.425</td>\n",
       "      <td>61</td>\n",
       "      <td>6.0032</td>\n",
       "      <td>12</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.12</td>\n",
       "      <td>16.665</td>\n",
       "      <td>66</td>\n",
       "      <td>8.9981</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10886 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       season  holiday  workingday  weather   temp   atemp  humidity  \\\n",
       "0           1        0           0        1   9.84  14.395        81   \n",
       "1           1        0           0        1   9.02  13.635        80   \n",
       "2           1        0           0        1   9.02  13.635        80   \n",
       "3           1        0           0        1   9.84  14.395        75   \n",
       "4           1        0           0        1   9.84  14.395        75   \n",
       "...       ...      ...         ...      ...    ...     ...       ...   \n",
       "10881       4        0           1        1  15.58  19.695        50   \n",
       "10882       4        0           1        1  14.76  17.425        57   \n",
       "10883       4        0           1        1  13.94  15.910        61   \n",
       "10884       4        0           1        1  13.94  17.425        61   \n",
       "10885       4        0           1        1  13.12  16.665        66   \n",
       "\n",
       "       windspeed  casual  registered  ...  weekday  month    0    1    2    3  \\\n",
       "0         0.0000       3          13  ...        5      1  1.0  0.0  0.0  0.0   \n",
       "1         0.0000       8          32  ...        5      1  1.0  0.0  0.0  0.0   \n",
       "2         0.0000       5          27  ...        5      1  1.0  0.0  0.0  0.0   \n",
       "3         0.0000       3          10  ...        5      1  1.0  0.0  0.0  0.0   \n",
       "4         0.0000       0           1  ...        5      1  1.0  0.0  0.0  0.0   \n",
       "...          ...     ...         ...  ...      ...    ...  ...  ...  ...  ...   \n",
       "10881    26.0027       7         329  ...        2     12  0.0  0.0  0.0  1.0   \n",
       "10882    15.0013      10         231  ...        2     12  0.0  0.0  0.0  1.0   \n",
       "10883    15.0013       4         164  ...        2     12  0.0  0.0  0.0  1.0   \n",
       "10884     6.0032      12         117  ...        2     12  0.0  0.0  0.0  1.0   \n",
       "10885     8.9981       4          84  ...        2     12  0.0  0.0  0.0  1.0   \n",
       "\n",
       "         0    1    2    3  \n",
       "0      1.0  0.0  0.0  0.0  \n",
       "1      1.0  0.0  0.0  0.0  \n",
       "2      1.0  0.0  0.0  0.0  \n",
       "3      1.0  0.0  0.0  0.0  \n",
       "4      1.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  ...  \n",
       "10881  1.0  0.0  0.0  0.0  \n",
       "10882  1.0  0.0  0.0  0.0  \n",
       "10883  1.0  0.0  0.0  0.0  \n",
       "10884  1.0  0.0  0.0  0.0  \n",
       "10885  1.0  0.0  0.0  0.0  \n",
       "\n",
       "[10886 rows x 22 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    data = data.sample(frac=1) #shuffle\n",
    "    test_index = int(data.shape[0] * 0.8)\n",
    "    y_training = data['count'][:test_index]\n",
    "    y_validation = data['count'][test_index:]\n",
    "    data = data.drop(['casual', 'registered', 'count', 'atemp', 'season'], axis=1)\n",
    "    return data[:test_index], y_training, data[test_index:], y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training, y_training, x_validation, y_validation = split_data(data_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9140</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.06</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4549</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.68</td>\n",
       "      <td>51</td>\n",
       "      <td>11.0014</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.86</td>\n",
       "      <td>36</td>\n",
       "      <td>26.0027</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17.22</td>\n",
       "      <td>88</td>\n",
       "      <td>7.0015</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7532</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.32</td>\n",
       "      <td>72</td>\n",
       "      <td>12.9980</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21.32</td>\n",
       "      <td>94</td>\n",
       "      <td>16.9979</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8662</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.70</td>\n",
       "      <td>74</td>\n",
       "      <td>7.0015</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6750</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18.86</td>\n",
       "      <td>82</td>\n",
       "      <td>11.0014</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6.56</td>\n",
       "      <td>86</td>\n",
       "      <td>7.0015</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9817</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.40</td>\n",
       "      <td>43</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8708 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      holiday  workingday  weather   temp  humidity  windspeed  hour  weekday  \\\n",
       "9140        0           1        2  27.06        89     0.0000     5        1   \n",
       "4549        0           1        1  19.68        51    11.0014    15        2   \n",
       "4598        0           1        1  18.86        36    26.0027    16        4   \n",
       "4228        0           0        1  17.22        88     7.0015     5        5   \n",
       "7532        0           0        1  21.32        72    12.9980     5        6   \n",
       "...       ...         ...      ...    ...       ...        ...   ...      ...   \n",
       "2168        0           1        3  21.32        94    16.9979     6        1   \n",
       "8662        0           1        1  28.70        74     7.0015     7        4   \n",
       "6750        0           0        2  18.86        82    11.0014    13        6   \n",
       "256         0           1        2   6.56        86     7.0015     1        2   \n",
       "9817        0           0        1  16.40        43     0.0000    10        5   \n",
       "\n",
       "      month    0    1    2    3    0    1    2    3  \n",
       "9140      9  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  \n",
       "4549     11  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  \n",
       "4598     11  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  \n",
       "4228     10  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  \n",
       "7532      5  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "2168      5  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "8662      8  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  \n",
       "6750      3  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "256       1  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "9817     10  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  \n",
       "\n",
       "[8708 rows x 17 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9140     38\n",
       "4549    187\n",
       "4598    290\n",
       "4228      8\n",
       "7532     12\n",
       "       ... \n",
       "2168     49\n",
       "8662    421\n",
       "6750    544\n",
       "256       6\n",
       "9817    347\n",
       "Name: count, Length: 8708, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mode(layer_units, input_size):\n",
    "    layers = []\n",
    "    layers.append(tf.keras.layers.Dense(units=layer_units[0], activation='relu', input_shape=(input_size,)))\n",
    "    for i in range(1,len(layer_units)):\n",
    "        layers.append(tf.keras.layers.Dense(units=layer_units[i], activation='relu'))\n",
    "    layers.append(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "    return tf.keras.models.Sequential(layers=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 18        \n",
      "=================================================================\n",
      "Total params: 324\n",
      "Trainable params: 324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_mode([17], 17)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss='mse')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8708 samples, validate on 2178 samples\n",
      "Epoch 1/100\n",
      "8708/8708 [==============================] - 0s 25us/sample - loss: 64299.7586 - val_loss: 57806.6644\n",
      "Epoch 2/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 55009.0552 - val_loss: 46420.7799\n",
      "Epoch 3/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 43916.2664 - val_loss: 37072.7123\n",
      "Epoch 4/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 36793.1004 - val_loss: 32402.6278\n",
      "Epoch 5/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 33406.6880 - val_loss: 30399.3144\n",
      "Epoch 6/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 31884.1402 - val_loss: 29305.5014\n",
      "Epoch 7/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 30723.6551 - val_loss: 28188.7652\n",
      "Epoch 8/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 29445.6379 - val_loss: 26922.3838\n",
      "Epoch 9/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 27931.6224 - val_loss: 25446.3813\n",
      "Epoch 10/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 26393.0562 - val_loss: 24170.0330\n",
      "Epoch 11/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 25102.5864 - val_loss: 23154.4808\n",
      "Epoch 12/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 24141.4749 - val_loss: 22449.9127\n",
      "Epoch 13/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 23459.5701 - val_loss: 21921.0358\n",
      "Epoch 14/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 22973.9544 - val_loss: 21581.0189\n",
      "Epoch 15/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 22640.5076 - val_loss: 21349.4592\n",
      "Epoch 16/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 22396.8233 - val_loss: 21183.6841\n",
      "Epoch 17/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 22228.4537 - val_loss: 21064.7618\n",
      "Epoch 18/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 22103.7892 - val_loss: 20980.2386\n",
      "Epoch 19/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 22003.6170 - val_loss: 20919.7231\n",
      "Epoch 20/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21919.2407 - val_loss: 20846.9774\n",
      "Epoch 21/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21850.7722 - val_loss: 20792.4314\n",
      "Epoch 22/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21796.8517 - val_loss: 20759.8472\n",
      "Epoch 23/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21747.0183 - val_loss: 20706.0747\n",
      "Epoch 24/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 21719.8787 - val_loss: 20688.1734\n",
      "Epoch 25/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 21706.6081 - val_loss: 20659.5443\n",
      "Epoch 26/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21653.3475 - val_loss: 20637.3491\n",
      "Epoch 27/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21629.9716 - val_loss: 20629.4275\n",
      "Epoch 28/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21619.9105 - val_loss: 20602.7868\n",
      "Epoch 29/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21590.4708 - val_loss: 20591.5628\n",
      "Epoch 30/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21572.6628 - val_loss: 20575.7542\n",
      "Epoch 31/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21576.8810 - val_loss: 20560.0301\n",
      "Epoch 32/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21539.6961 - val_loss: 20552.4250\n",
      "Epoch 33/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21529.7722 - val_loss: 20538.7757\n",
      "Epoch 34/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21517.1942 - val_loss: 20524.8437\n",
      "Epoch 35/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21505.1460 - val_loss: 20533.4711\n",
      "Epoch 36/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 21514.1489 - val_loss: 20500.9674\n",
      "Epoch 37/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 21473.0410 - val_loss: 20490.7804\n",
      "Epoch 38/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 21463.3807 - val_loss: 20482.7459\n",
      "Epoch 39/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 21457.4019 - val_loss: 20473.0873\n",
      "Epoch 40/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 21431.3061 - val_loss: 20459.8537\n",
      "Epoch 41/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 21423.5749 - val_loss: 20450.4700\n",
      "Epoch 42/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 21407.8512 - val_loss: 20426.4186\n",
      "Epoch 43/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21398.6077 - val_loss: 20417.2596\n",
      "Epoch 44/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21388.4028 - val_loss: 20426.4155\n",
      "Epoch 45/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21440.8600 - val_loss: 20432.9004\n",
      "Epoch 46/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21380.1994 - val_loss: 20388.8933\n",
      "Epoch 47/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 21344.7918 - val_loss: 20376.0250\n",
      "Epoch 48/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21329.1575 - val_loss: 20370.8998\n",
      "Epoch 49/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21316.9501 - val_loss: 20362.8306\n",
      "Epoch 50/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21312.1260 - val_loss: 20350.4659\n",
      "Epoch 51/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21270.7437 - val_loss: 20326.6903\n",
      "Epoch 52/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21252.9115 - val_loss: 20316.1369\n",
      "Epoch 53/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 21239.2482 - val_loss: 20314.9883\n",
      "Epoch 54/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 21268.6556 - val_loss: 20257.6814\n",
      "Epoch 55/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 21188.4789 - val_loss: 20233.8121\n",
      "Epoch 56/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21158.7681 - val_loss: 20194.5029\n",
      "Epoch 57/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21126.2936 - val_loss: 20163.1876\n",
      "Epoch 58/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 21098.2794 - val_loss: 20140.5347\n",
      "Epoch 59/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 21082.8269 - val_loss: 20126.5972\n",
      "Epoch 60/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 21054.8910 - val_loss: 20089.2943\n",
      "Epoch 61/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 21022.2847 - val_loss: 20063.8045\n",
      "Epoch 62/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 21006.0943 - val_loss: 20037.1036\n",
      "Epoch 63/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 20975.5075 - val_loss: 20012.1109\n",
      "Epoch 64/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 20941.5887 - val_loss: 20011.1011\n",
      "Epoch 65/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 20923.0948 - val_loss: 19962.0693\n",
      "Epoch 66/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 20893.4153 - val_loss: 19929.9399\n",
      "Epoch 67/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 20864.3921 - val_loss: 19911.4762\n",
      "Epoch 68/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 20835.9238 - val_loss: 19875.5406\n",
      "Epoch 69/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 20801.1258 - val_loss: 19851.1000\n",
      "Epoch 70/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 20782.3142 - val_loss: 19815.3072\n",
      "Epoch 71/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 20742.9666 - val_loss: 19783.3974\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8708/8708 [==============================] - 0s 10us/sample - loss: 20714.0883 - val_loss: 19760.8469\n",
      "Epoch 73/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 20672.5543 - val_loss: 19727.4049\n",
      "Epoch 74/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 20672.4274 - val_loss: 19695.3791\n",
      "Epoch 75/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 20601.0232 - val_loss: 19642.5828\n",
      "Epoch 76/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 20561.2305 - val_loss: 19642.0364\n",
      "Epoch 77/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 20521.8780 - val_loss: 19575.6839\n",
      "Epoch 78/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 20508.9238 - val_loss: 19532.2251\n",
      "Epoch 79/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 20441.4000 - val_loss: 19509.5856\n",
      "Epoch 80/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 20412.1597 - val_loss: 19457.4022\n",
      "Epoch 81/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 20366.4138 - val_loss: 19423.2723\n",
      "Epoch 82/100\n",
      "8708/8708 [==============================] - 0s 7us/sample - loss: 20338.0947 - val_loss: 19381.9227\n",
      "Epoch 83/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 20292.3269 - val_loss: 19341.2098\n",
      "Epoch 84/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 20238.0701 - val_loss: 19352.6829\n",
      "Epoch 85/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 20219.4991 - val_loss: 19264.8835\n",
      "Epoch 86/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 20179.6278 - val_loss: 19218.1374\n",
      "Epoch 87/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 20130.8152 - val_loss: 19171.3049\n",
      "Epoch 88/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 20079.2936 - val_loss: 19127.6584\n",
      "Epoch 89/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 20046.6065 - val_loss: 19080.9504\n",
      "Epoch 90/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 19993.8380 - val_loss: 19041.8264\n",
      "Epoch 91/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 19946.3572 - val_loss: 18997.8874\n",
      "Epoch 92/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 19911.4912 - val_loss: 18970.4409\n",
      "Epoch 93/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 19876.0180 - val_loss: 18919.7884\n",
      "Epoch 94/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 19824.5884 - val_loss: 18866.4173\n",
      "Epoch 95/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 19785.8415 - val_loss: 18825.7913\n",
      "Epoch 96/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 19749.1552 - val_loss: 18776.7057\n",
      "Epoch 97/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 19701.4145 - val_loss: 18732.5627\n",
      "Epoch 98/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 19663.9454 - val_loss: 18729.8486\n",
      "Epoch 99/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 19614.3784 - val_loss: 18647.3742\n",
      "Epoch 100/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 19574.7011 - val_loss: 18609.5433\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_training, y_training, epochs=100, batch_size=128, validation_data=(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss ~ 19574"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "increase lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 18        \n",
      "=================================================================\n",
      "Total params: 324\n",
      "Trainable params: 324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8708 samples, validate on 2178 samples\n",
      "Epoch 1/100\n",
      "8708/8708 [==============================] - 0s 25us/sample - loss: 45016.8528 - val_loss: 30423.0082\n",
      "Epoch 2/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 29163.0377 - val_loss: 24079.5859\n",
      "Epoch 3/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 23550.9003 - val_loss: 21094.9832\n",
      "Epoch 4/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 22018.8585 - val_loss: 20859.0315\n",
      "Epoch 5/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21884.3468 - val_loss: 20632.6563\n",
      "Epoch 6/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21540.0564 - val_loss: 20430.6622\n",
      "Epoch 7/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21484.8640 - val_loss: 20272.5019\n",
      "Epoch 8/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21181.5897 - val_loss: 20457.7176\n",
      "Epoch 9/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21261.9547 - val_loss: 19894.9049\n",
      "Epoch 10/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 20740.3585 - val_loss: 19597.2345\n",
      "Epoch 11/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 20344.4863 - val_loss: 19156.3870\n",
      "Epoch 12/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 19964.3969 - val_loss: 18710.4130\n",
      "Epoch 13/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 19601.0666 - val_loss: 18849.8885\n",
      "Epoch 14/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 19427.9959 - val_loss: 18077.1428\n",
      "Epoch 15/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 18947.3108 - val_loss: 17912.6711\n",
      "Epoch 16/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 18619.1364 - val_loss: 17545.1393\n",
      "Epoch 17/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 18377.6139 - val_loss: 17377.8573\n",
      "Epoch 18/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 18163.9018 - val_loss: 17098.0966\n",
      "Epoch 19/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 17924.0964 - val_loss: 16934.0850\n",
      "Epoch 20/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 17730.7009 - val_loss: 16757.5159\n",
      "Epoch 21/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 17605.8001 - val_loss: 16647.7095\n",
      "Epoch 22/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 17425.8823 - val_loss: 16563.6462\n",
      "Epoch 23/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 17362.1062 - val_loss: 16385.5294\n",
      "Epoch 24/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 17237.2508 - val_loss: 16314.6912\n",
      "Epoch 25/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 17474.6447 - val_loss: 16285.0100\n",
      "Epoch 26/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 17075.4503 - val_loss: 16209.7770\n",
      "Epoch 27/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16974.2724 - val_loss: 16330.2916\n",
      "Epoch 28/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16929.5730 - val_loss: 16021.3225\n",
      "Epoch 29/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 16935.5607 - val_loss: 16215.4079\n",
      "Epoch 30/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16743.2951 - val_loss: 15933.0505\n",
      "Epoch 31/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16669.9639 - val_loss: 15853.8148\n",
      "Epoch 32/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16698.5404 - val_loss: 15846.9409\n",
      "Epoch 33/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 16561.8364 - val_loss: 15944.3702\n",
      "Epoch 34/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 16517.8714 - val_loss: 15791.6612\n",
      "Epoch 35/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16438.8851 - val_loss: 15790.7867\n",
      "Epoch 36/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16580.1044 - val_loss: 15584.1891\n",
      "Epoch 37/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 16420.5880 - val_loss: 15590.8600\n",
      "Epoch 38/100\n",
      "8708/8708 [==============================] - ETA: 0s - loss: 16059.771 - 0s 10us/sample - loss: 16388.9581 - val_loss: 15559.3477\n",
      "Epoch 39/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16308.5171 - val_loss: 15425.1680\n",
      "Epoch 40/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16186.7483 - val_loss: 15545.4948\n",
      "Epoch 41/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 16223.8201 - val_loss: 15513.4077\n",
      "Epoch 42/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16190.4070 - val_loss: 15375.4970\n",
      "Epoch 43/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16108.1443 - val_loss: 15954.5926\n",
      "Epoch 44/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16127.0225 - val_loss: 15299.8281\n",
      "Epoch 45/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15982.7777 - val_loss: 15313.8753\n",
      "Epoch 46/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15999.2500 - val_loss: 15317.5161\n",
      "Epoch 47/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15954.0212 - val_loss: 15224.5856\n",
      "Epoch 48/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15971.3849 - val_loss: 15306.7449\n",
      "Epoch 49/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15891.7877 - val_loss: 15105.5527\n",
      "Epoch 50/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15841.8543 - val_loss: 15102.0402\n",
      "Epoch 51/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15779.0468 - val_loss: 15202.5198\n",
      "Epoch 52/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15865.6968 - val_loss: 15034.5025\n",
      "Epoch 53/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15991.5209 - val_loss: 15022.7377\n",
      "Epoch 54/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15797.3431 - val_loss: 15541.3165\n",
      "Epoch 55/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15955.9277 - val_loss: 14999.7819\n",
      "Epoch 56/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15701.9713 - val_loss: 14946.6888\n",
      "Epoch 57/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15665.2319 - val_loss: 15137.1257\n",
      "Epoch 58/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15754.0159 - val_loss: 15372.3995\n",
      "Epoch 59/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15655.5808 - val_loss: 15144.3730\n",
      "Epoch 60/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15628.1151 - val_loss: 15139.0999\n",
      "Epoch 61/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15641.1201 - val_loss: 14907.1546\n",
      "Epoch 62/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15601.1952 - val_loss: 14863.9514\n",
      "Epoch 63/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15626.1441 - val_loss: 15239.6918\n",
      "Epoch 64/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15604.2064 - val_loss: 15375.0385\n",
      "Epoch 65/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15727.9550 - val_loss: 14839.2034\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15532.9497 - val_loss: 15199.5815\n",
      "Epoch 67/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15624.7907 - val_loss: 15019.0192\n",
      "Epoch 68/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15655.4889 - val_loss: 14856.6068\n",
      "Epoch 69/100\n",
      "8708/8708 [==============================] - 0s 7us/sample - loss: 15612.4520 - val_loss: 14903.2705\n",
      "Epoch 70/100\n",
      "8708/8708 [==============================] - 0s 7us/sample - loss: 15541.7694 - val_loss: 14829.9968\n",
      "Epoch 71/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15637.9990 - val_loss: 15176.0267\n",
      "Epoch 72/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15707.0150 - val_loss: 14818.3743\n",
      "Epoch 73/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15490.4812 - val_loss: 15116.8101\n",
      "Epoch 74/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15554.0538 - val_loss: 15271.3612\n",
      "Epoch 75/100\n",
      "8708/8708 [==============================] - 0s 7us/sample - loss: 15533.8672 - val_loss: 14789.1251\n",
      "Epoch 76/100\n",
      "8708/8708 [==============================] - 0s 7us/sample - loss: 15414.3242 - val_loss: 14856.0472\n",
      "Epoch 77/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15414.4434 - val_loss: 14787.5474\n",
      "Epoch 78/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15403.6854 - val_loss: 14972.9736\n",
      "Epoch 79/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15362.5485 - val_loss: 14941.5094\n",
      "Epoch 80/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15577.8653 - val_loss: 14747.6087\n",
      "Epoch 81/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15402.7634 - val_loss: 14772.5388\n",
      "Epoch 82/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15414.6080 - val_loss: 14656.6801\n",
      "Epoch 83/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15352.9283 - val_loss: 14660.9786\n",
      "Epoch 84/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15367.9485 - val_loss: 14724.8531\n",
      "Epoch 85/100\n",
      "8708/8708 [==============================] - 0s 7us/sample - loss: 15336.5723 - val_loss: 14687.6368\n",
      "Epoch 86/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15313.6982 - val_loss: 14661.0757\n",
      "Epoch 87/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15354.9581 - val_loss: 14656.0201\n",
      "Epoch 88/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15329.2892 - val_loss: 15169.2553\n",
      "Epoch 89/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15388.6576 - val_loss: 15109.4693\n",
      "Epoch 90/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15372.5954 - val_loss: 14678.0872\n",
      "Epoch 91/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15257.8395 - val_loss: 14705.0379\n",
      "Epoch 92/100\n",
      "8708/8708 [==============================] - 0s 7us/sample - loss: 15331.6090 - val_loss: 14568.5969\n",
      "Epoch 93/100\n",
      "8708/8708 [==============================] - 0s 7us/sample - loss: 15224.2722 - val_loss: 15816.0237\n",
      "Epoch 94/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15598.5657 - val_loss: 14525.6521\n",
      "Epoch 95/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15203.9193 - val_loss: 14469.3500\n",
      "Epoch 96/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15226.3173 - val_loss: 14561.4333\n",
      "Epoch 97/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15188.6283 - val_loss: 14457.1995\n",
      "Epoch 98/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15158.1965 - val_loss: 14971.5834\n",
      "Epoch 99/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15266.9463 - val_loss: 14520.3250\n",
      "Epoch 100/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15161.0125 - val_loss: 14428.5886\n"
     ]
    }
   ],
   "source": [
    "model_lr = build_mode([17], 17)\n",
    "\n",
    "model_lr.compile(optimizer=tf.keras.optimizers.Adam(lr=0.008), loss='mse')\n",
    "model_lr.summary()\n",
    "history = model_lr.fit(x_training, y_training, epochs=100, batch_size=128, validation_data=(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss ~ 15161"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data):\n",
    "    ss = StandardScaler()\n",
    "    data = data.values.reshape(-1, 1)\n",
    "    scaler = ss.fit(data)\n",
    "    data = ss.transform(data)\n",
    "    return data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>...</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10881</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.58</td>\n",
       "      <td>19.695</td>\n",
       "      <td>50</td>\n",
       "      <td>26.0027</td>\n",
       "      <td>7</td>\n",
       "      <td>329</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10882</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.76</td>\n",
       "      <td>17.425</td>\n",
       "      <td>57</td>\n",
       "      <td>15.0013</td>\n",
       "      <td>10</td>\n",
       "      <td>231</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10883</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.94</td>\n",
       "      <td>15.910</td>\n",
       "      <td>61</td>\n",
       "      <td>15.0013</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10884</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.94</td>\n",
       "      <td>17.425</td>\n",
       "      <td>61</td>\n",
       "      <td>6.0032</td>\n",
       "      <td>12</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.12</td>\n",
       "      <td>16.665</td>\n",
       "      <td>66</td>\n",
       "      <td>8.9981</td>\n",
       "      <td>4</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10886 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       season  holiday  workingday  weather   temp   atemp  humidity  \\\n",
       "0           1        0           0        1   9.84  14.395        81   \n",
       "1           1        0           0        1   9.02  13.635        80   \n",
       "2           1        0           0        1   9.02  13.635        80   \n",
       "3           1        0           0        1   9.84  14.395        75   \n",
       "4           1        0           0        1   9.84  14.395        75   \n",
       "...       ...      ...         ...      ...    ...     ...       ...   \n",
       "10881       4        0           1        1  15.58  19.695        50   \n",
       "10882       4        0           1        1  14.76  17.425        57   \n",
       "10883       4        0           1        1  13.94  15.910        61   \n",
       "10884       4        0           1        1  13.94  17.425        61   \n",
       "10885       4        0           1        1  13.12  16.665        66   \n",
       "\n",
       "       windspeed  casual  registered  ...  weekday  month    0    1    2    3  \\\n",
       "0         0.0000       3          13  ...        5      1  1.0  0.0  0.0  0.0   \n",
       "1         0.0000       8          32  ...        5      1  1.0  0.0  0.0  0.0   \n",
       "2         0.0000       5          27  ...        5      1  1.0  0.0  0.0  0.0   \n",
       "3         0.0000       3          10  ...        5      1  1.0  0.0  0.0  0.0   \n",
       "4         0.0000       0           1  ...        5      1  1.0  0.0  0.0  0.0   \n",
       "...          ...     ...         ...  ...      ...    ...  ...  ...  ...  ...   \n",
       "10881    26.0027       7         329  ...        2     12  0.0  0.0  0.0  1.0   \n",
       "10882    15.0013      10         231  ...        2     12  0.0  0.0  0.0  1.0   \n",
       "10883    15.0013       4         164  ...        2     12  0.0  0.0  0.0  1.0   \n",
       "10884     6.0032      12         117  ...        2     12  0.0  0.0  0.0  1.0   \n",
       "10885     8.9981       4          84  ...        2     12  0.0  0.0  0.0  1.0   \n",
       "\n",
       "         0    1    2    3  \n",
       "0      1.0  0.0  0.0  0.0  \n",
       "1      1.0  0.0  0.0  0.0  \n",
       "2      1.0  0.0  0.0  0.0  \n",
       "3      1.0  0.0  0.0  0.0  \n",
       "4      1.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  ...  \n",
       "10881  1.0  0.0  0.0  0.0  \n",
       "10882  1.0  0.0  0.0  0.0  \n",
       "10883  1.0  0.0  0.0  0.0  \n",
       "10884  1.0  0.0  0.0  0.0  \n",
       "10885  1.0  0.0  0.0  0.0  \n",
       "\n",
       "[10886 rows x 22 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9140</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.869881</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4549</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.076695</td>\n",
       "      <td>51</td>\n",
       "      <td>11.0014</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.181870</td>\n",
       "      <td>36</td>\n",
       "      <td>26.0027</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.392221</td>\n",
       "      <td>88</td>\n",
       "      <td>7.0015</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7532</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.133655</td>\n",
       "      <td>72</td>\n",
       "      <td>12.9980</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.133655</td>\n",
       "      <td>94</td>\n",
       "      <td>16.9979</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8662</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.080231</td>\n",
       "      <td>74</td>\n",
       "      <td>7.0015</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6750</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.181870</td>\n",
       "      <td>82</td>\n",
       "      <td>11.0014</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.759497</td>\n",
       "      <td>86</td>\n",
       "      <td>7.0015</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9817</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.497396</td>\n",
       "      <td>43</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8708 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      holiday  workingday  weather      temp  humidity  windspeed  hour  \\\n",
       "9140        0           1        2  0.869881        89     0.0000     5   \n",
       "4549        0           1        1 -0.076695        51    11.0014    15   \n",
       "4598        0           1        1 -0.181870        36    26.0027    16   \n",
       "4228        0           0        1 -0.392221        88     7.0015     5   \n",
       "7532        0           0        1  0.133655        72    12.9980     5   \n",
       "...       ...         ...      ...       ...       ...        ...   ...   \n",
       "2168        0           1        3  0.133655        94    16.9979     6   \n",
       "8662        0           1        1  1.080231        74     7.0015     7   \n",
       "6750        0           0        2 -0.181870        82    11.0014    13   \n",
       "256         0           1        2 -1.759497        86     7.0015     1   \n",
       "9817        0           0        1 -0.497396        43     0.0000    10   \n",
       "\n",
       "      weekday  month    0    1    2    3    0    1    2    3  \n",
       "9140        1      9  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  \n",
       "4549        2     11  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  \n",
       "4598        4     11  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  \n",
       "4228        5     10  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  \n",
       "7532        6      5  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "...       ...    ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "2168        1      5  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "8662        4      8  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  \n",
       "6750        6      3  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "256         2      1  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "9817        5     10  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  \n",
       "\n",
       "[8708 rows x 17 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_temp, temp_scaler = scale_data(x_training['temp'])\n",
    "x_scale_temp = x_training.copy()\n",
    "x_scale_temp['temp'] = scaled_temp\n",
    "val_scale_temp = temp_scaler.transform(x_validation['temp'].values.reshape(-1,1))\n",
    "x_val_temp = x_validation.copy()\n",
    "x_val_temp['temp'] = val_scale_temp\n",
    "x_scale_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 17)                306       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 18        \n",
      "=================================================================\n",
      "Total params: 324\n",
      "Trainable params: 324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8708 samples, validate on 2178 samples\n",
      "Epoch 1/100\n",
      "8708/8708 [==============================] - 0s 25us/sample - loss: 43463.2584 - val_loss: 31984.3873\n",
      "Epoch 2/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 31035.7529 - val_loss: 25767.6986\n",
      "Epoch 3/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 25934.9418 - val_loss: 22987.0167\n",
      "Epoch 4/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 23976.2480 - val_loss: 21946.2155\n",
      "Epoch 5/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 22923.1575 - val_loss: 21196.1204\n",
      "Epoch 6/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 22164.3617 - val_loss: 20626.3591\n",
      "Epoch 7/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21640.7706 - val_loss: 20136.0540\n",
      "Epoch 8/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 21055.5025 - val_loss: 19769.9149\n",
      "Epoch 9/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 20635.4560 - val_loss: 19481.2783\n",
      "Epoch 10/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 20298.5560 - val_loss: 19240.4705\n",
      "Epoch 11/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 19978.5394 - val_loss: 19328.9963\n",
      "Epoch 12/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 19789.1918 - val_loss: 18483.1628\n",
      "Epoch 13/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 19401.5559 - val_loss: 18359.1693\n",
      "Epoch 14/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 19332.1252 - val_loss: 18113.5123\n",
      "Epoch 15/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 18966.9447 - val_loss: 17833.8230\n",
      "Epoch 16/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 18843.1930 - val_loss: 17850.5767\n",
      "Epoch 17/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 18699.7897 - val_loss: 17701.7193\n",
      "Epoch 18/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 18546.6771 - val_loss: 17391.3599\n",
      "Epoch 19/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 18392.1738 - val_loss: 17673.2428\n",
      "Epoch 20/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 18531.6593 - val_loss: 17337.9672\n",
      "Epoch 21/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 18191.3175 - val_loss: 17315.2231\n",
      "Epoch 22/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 18203.8818 - val_loss: 17092.0749\n",
      "Epoch 23/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 17991.5390 - val_loss: 16972.1793\n",
      "Epoch 24/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 17965.9322 - val_loss: 17191.1973\n",
      "Epoch 25/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 18012.5016 - val_loss: 16833.0273\n",
      "Epoch 26/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 17844.0438 - val_loss: 16808.3823\n",
      "Epoch 27/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 17776.1230 - val_loss: 16694.8642\n",
      "Epoch 28/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 17680.4475 - val_loss: 16773.6071\n",
      "Epoch 29/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 17634.4043 - val_loss: 16613.4990\n",
      "Epoch 30/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 17655.6511 - val_loss: 16569.8100\n",
      "Epoch 31/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 17520.6312 - val_loss: 16566.6324\n",
      "Epoch 32/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 17420.5201 - val_loss: 16418.9356\n",
      "Epoch 33/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 17332.3830 - val_loss: 16417.9131\n",
      "Epoch 34/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 17336.8238 - val_loss: 16826.1374\n",
      "Epoch 35/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 17458.8905 - val_loss: 16250.4581\n",
      "Epoch 36/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 17170.8764 - val_loss: 16258.6810\n",
      "Epoch 37/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 17144.6337 - val_loss: 16483.8287\n",
      "Epoch 38/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 17178.4884 - val_loss: 16129.6752\n",
      "Epoch 39/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16997.4635 - val_loss: 16071.1055\n",
      "Epoch 40/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 16932.2608 - val_loss: 16060.9051\n",
      "Epoch 41/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 17002.5903 - val_loss: 16301.3011\n",
      "Epoch 42/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 16879.9086 - val_loss: 15855.7088\n",
      "Epoch 43/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 16800.3449 - val_loss: 15841.0932\n",
      "Epoch 44/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 16745.2728 - val_loss: 15917.4104\n",
      "Epoch 45/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 16804.3694 - val_loss: 15796.5267\n",
      "Epoch 46/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 16632.7300 - val_loss: 15685.3566\n",
      "Epoch 47/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 16653.8019 - val_loss: 15833.2940\n",
      "Epoch 48/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 16554.1883 - val_loss: 15681.8392\n",
      "Epoch 49/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 16491.9074 - val_loss: 15595.4927\n",
      "Epoch 50/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16439.5520 - val_loss: 15537.8982\n",
      "Epoch 51/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 16346.4808 - val_loss: 15616.6209\n",
      "Epoch 52/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 16324.9731 - val_loss: 15424.2715\n",
      "Epoch 53/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16450.3183 - val_loss: 15399.0624\n",
      "Epoch 54/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 16267.3217 - val_loss: 15361.7317\n",
      "Epoch 55/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 16224.5469 - val_loss: 15296.0331\n",
      "Epoch 56/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16218.7145 - val_loss: 15283.2029\n",
      "Epoch 57/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16132.4107 - val_loss: 15617.5939\n",
      "Epoch 58/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 16205.3075 - val_loss: 15253.0977\n",
      "Epoch 59/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 16102.6870 - val_loss: 15209.3140\n",
      "Epoch 60/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 16047.5226 - val_loss: 15120.5483\n",
      "Epoch 61/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15929.2488 - val_loss: 15115.0833\n",
      "Epoch 62/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 16004.9085 - val_loss: 15207.5771\n",
      "Epoch 63/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15986.0671 - val_loss: 15033.6436\n",
      "Epoch 64/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15847.7259 - val_loss: 15092.5207\n",
      "Epoch 65/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15849.4802 - val_loss: 14962.1319\n",
      "Epoch 66/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 15857.7738 - val_loss: 14955.5081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 15737.4843 - val_loss: 14909.1300\n",
      "Epoch 68/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 15725.5662 - val_loss: 15041.7953\n",
      "Epoch 69/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15792.1266 - val_loss: 15670.9788\n",
      "Epoch 70/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 16036.0717 - val_loss: 14965.8317\n",
      "Epoch 71/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15742.7885 - val_loss: 14828.1688\n",
      "Epoch 72/100\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 15578.3555 - val_loss: 14785.8374\n",
      "Epoch 73/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15578.5856 - val_loss: 14731.7541\n",
      "Epoch 74/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15521.2231 - val_loss: 14710.2920\n",
      "Epoch 75/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15506.0850 - val_loss: 14633.8449\n",
      "Epoch 76/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15583.5245 - val_loss: 14639.0086\n",
      "Epoch 77/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15421.0428 - val_loss: 14583.6961\n",
      "Epoch 78/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15419.8632 - val_loss: 14501.8659\n",
      "Epoch 79/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15294.4317 - val_loss: 14498.5572\n",
      "Epoch 80/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15303.9957 - val_loss: 14561.6177\n",
      "Epoch 81/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15270.3706 - val_loss: 14307.5829\n",
      "Epoch 82/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15144.8449 - val_loss: 14376.4224\n",
      "Epoch 83/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15133.2174 - val_loss: 14245.8326\n",
      "Epoch 84/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15100.0816 - val_loss: 14189.7191\n",
      "Epoch 85/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 15030.1171 - val_loss: 14191.3173\n",
      "Epoch 86/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 15116.2424 - val_loss: 14131.0665\n",
      "Epoch 87/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 14991.7114 - val_loss: 14122.0962\n",
      "Epoch 88/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 14895.2896 - val_loss: 14107.0331\n",
      "Epoch 89/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 14926.8423 - val_loss: 14044.3462\n",
      "Epoch 90/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 14865.3194 - val_loss: 14040.5862\n",
      "Epoch 91/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 14883.8983 - val_loss: 13992.0543\n",
      "Epoch 92/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 14760.6183 - val_loss: 14064.1812\n",
      "Epoch 93/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 14844.0014 - val_loss: 13920.5439\n",
      "Epoch 94/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 14694.1301 - val_loss: 13939.1954\n",
      "Epoch 95/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 14652.7037 - val_loss: 13847.6684\n",
      "Epoch 96/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 14615.3077 - val_loss: 13957.5792\n",
      "Epoch 97/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 14641.2790 - val_loss: 13875.3454\n",
      "Epoch 98/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 14572.9547 - val_loss: 13960.8437\n",
      "Epoch 99/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 14604.1627 - val_loss: 13849.0989\n",
      "Epoch 100/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 14538.2498 - val_loss: 13712.5052\n"
     ]
    }
   ],
   "source": [
    "model_lr = build_mode([17], 17)\n",
    "\n",
    "model_lr.compile(optimizer=tf.keras.optimizers.Adam(lr=0.008), loss='mse')\n",
    "model_lr.summary()\n",
    "history = model_lr.fit(x_scale_temp, y_training, epochs=100, batch_size=128, validation_data=(x_val_temp, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss ~ 14538   better, not much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 128)               2304      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 10,625\n",
      "Trainable params: 10,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8708 samples, validate on 2178 samples\n",
      "Epoch 1/100\n",
      "8708/8708 [==============================] - 0s 27us/sample - loss: 32548.9215 - val_loss: 22028.1403\n",
      "Epoch 2/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 22528.5973 - val_loss: 20209.3316\n",
      "Epoch 3/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 20962.7102 - val_loss: 19102.1670\n",
      "Epoch 4/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 19798.5104 - val_loss: 22676.0893\n",
      "Epoch 5/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 19080.9053 - val_loss: 17024.8651\n",
      "Epoch 6/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 17559.8310 - val_loss: 16200.0132\n",
      "Epoch 7/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 16873.8292 - val_loss: 16931.7371\n",
      "Epoch 8/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 16721.2983 - val_loss: 15426.4750\n",
      "Epoch 9/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 16045.7189 - val_loss: 16920.0502\n",
      "Epoch 10/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 16218.8733 - val_loss: 16484.8924\n",
      "Epoch 11/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 15629.2399 - val_loss: 14442.8023\n",
      "Epoch 12/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 14725.3388 - val_loss: 15387.3884\n",
      "Epoch 13/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 14764.8973 - val_loss: 15399.2326\n",
      "Epoch 14/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 14201.3337 - val_loss: 13154.0804\n",
      "Epoch 15/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 13693.7116 - val_loss: 12897.0718\n",
      "Epoch 16/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 13452.7476 - val_loss: 13165.7724\n",
      "Epoch 17/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 13145.5888 - val_loss: 17468.8177\n",
      "Epoch 18/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 13224.1266 - val_loss: 15448.3136\n",
      "Epoch 19/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 12478.2584 - val_loss: 11823.8733\n",
      "Epoch 20/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 12038.0291 - val_loss: 11337.3333\n",
      "Epoch 21/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 11926.4262 - val_loss: 11936.7843\n",
      "Epoch 22/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 12193.0364 - val_loss: 10861.5463\n",
      "Epoch 23/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 11416.4885 - val_loss: 11194.9023\n",
      "Epoch 24/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 11726.7958 - val_loss: 10616.2929\n",
      "Epoch 25/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 10788.0551 - val_loss: 10211.9473\n",
      "Epoch 26/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 10566.9457 - val_loss: 11048.3895\n",
      "Epoch 27/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 11260.1015 - val_loss: 10326.2705\n",
      "Epoch 28/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 10500.6171 - val_loss: 13593.2636\n",
      "Epoch 29/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 11181.9216 - val_loss: 9966.4713\n",
      "Epoch 30/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 9879.8506 - val_loss: 11465.7391\n",
      "Epoch 31/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 9925.2077 - val_loss: 11779.2879\n",
      "Epoch 32/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 9983.5083 - val_loss: 9172.2834\n",
      "Epoch 33/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 9654.0970 - val_loss: 11852.1831\n",
      "Epoch 34/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 10161.9321 - val_loss: 9904.9114\n",
      "Epoch 35/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 9387.3601 - val_loss: 8779.0486\n",
      "Epoch 36/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 9284.8465 - val_loss: 8887.4982\n",
      "Epoch 37/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 9277.1489 - val_loss: 9499.1097\n",
      "Epoch 38/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 9251.8558 - val_loss: 8694.5661\n",
      "Epoch 39/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 8425.5836 - val_loss: 9974.0814\n",
      "Epoch 40/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 8821.9244 - val_loss: 8896.6925\n",
      "Epoch 41/100\n",
      "8708/8708 [==============================] - 0s 14us/sample - loss: 8555.1561 - val_loss: 8289.6765\n",
      "Epoch 42/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 8267.9257 - val_loss: 8643.8370\n",
      "Epoch 43/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 8712.7049 - val_loss: 8228.2380\n",
      "Epoch 44/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 8202.0574 - val_loss: 9592.4478\n",
      "Epoch 45/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 8370.7213 - val_loss: 9214.0763\n",
      "Epoch 46/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 8272.3235 - val_loss: 9768.8976\n",
      "Epoch 47/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 8347.8587 - val_loss: 7954.3053\n",
      "Epoch 48/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7754.1383 - val_loss: 19867.4432\n",
      "Epoch 49/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 11791.6807 - val_loss: 8230.7017\n",
      "Epoch 50/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 8186.9534 - val_loss: 10506.1620\n",
      "Epoch 51/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 8401.3513 - val_loss: 8193.5595\n",
      "Epoch 52/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 8285.9200 - val_loss: 7678.2942\n",
      "Epoch 53/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 7985.5835 - val_loss: 7829.8514\n",
      "Epoch 54/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7618.0191 - val_loss: 7743.6946\n",
      "Epoch 55/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 8087.2132 - val_loss: 18867.6966\n",
      "Epoch 56/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 8744.2405 - val_loss: 7626.5364\n",
      "Epoch 57/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 7459.6696 - val_loss: 7327.2746\n",
      "Epoch 58/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 7774.8395 - val_loss: 7817.5062\n",
      "Epoch 59/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 7663.1709 - val_loss: 6979.9950\n",
      "Epoch 60/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 6880.1711 - val_loss: 7249.8831\n",
      "Epoch 61/100\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 7262.9828 - val_loss: 6981.7706\n",
      "Epoch 62/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 7075.7918 - val_loss: 7456.8847\n",
      "Epoch 63/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 7008.3888 - val_loss: 7998.7827\n",
      "Epoch 64/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7397.2147 - val_loss: 7486.5262\n",
      "Epoch 65/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7001.6146 - val_loss: 6669.2133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7190.9081 - val_loss: 6321.2410\n",
      "Epoch 67/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6908.9159 - val_loss: 7593.2038\n",
      "Epoch 68/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 6838.5689 - val_loss: 14349.3553\n",
      "Epoch 69/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 12132.9441 - val_loss: 8946.9982\n",
      "Epoch 70/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 8100.5705 - val_loss: 7230.0742\n",
      "Epoch 71/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6906.7819 - val_loss: 9228.3931\n",
      "Epoch 72/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7798.3061 - val_loss: 8580.4573\n",
      "Epoch 73/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 7901.9064 - val_loss: 7611.2198\n",
      "Epoch 74/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 6597.1333 - val_loss: 7361.3030\n",
      "Epoch 75/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 6494.4310 - val_loss: 7370.1821\n",
      "Epoch 76/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6334.8262 - val_loss: 7097.1206\n",
      "Epoch 77/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7248.6370 - val_loss: 6046.3396\n",
      "Epoch 78/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6284.0870 - val_loss: 6433.8852\n",
      "Epoch 79/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6273.5138 - val_loss: 6910.1558\n",
      "Epoch 80/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6290.4198 - val_loss: 6475.7891\n",
      "Epoch 81/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6672.7184 - val_loss: 6379.8873\n",
      "Epoch 82/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 6367.2778 - val_loss: 6135.2944\n",
      "Epoch 83/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 5960.4611 - val_loss: 6469.2074\n",
      "Epoch 84/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 6962.4527 - val_loss: 7043.7157\n",
      "Epoch 85/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 6453.7810 - val_loss: 6462.4618\n",
      "Epoch 86/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6110.2276 - val_loss: 6825.8676\n",
      "Epoch 87/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6814.6631 - val_loss: 8646.3027\n",
      "Epoch 88/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 6234.7104 - val_loss: 6234.6274\n",
      "Epoch 89/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 6568.9255 - val_loss: 6228.2169\n",
      "Epoch 90/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6156.4771 - val_loss: 7330.2710\n",
      "Epoch 91/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 6293.0827 - val_loss: 6960.1467\n",
      "Epoch 92/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6125.1214 - val_loss: 5898.9556\n",
      "Epoch 93/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 6417.5378 - val_loss: 8641.8640\n",
      "Epoch 94/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 9569.0445 - val_loss: 8477.6992\n",
      "Epoch 95/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7808.1582 - val_loss: 7782.9994\n",
      "Epoch 96/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 7519.6484 - val_loss: 7932.6428\n",
      "Epoch 97/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7411.5138 - val_loss: 16043.1684\n",
      "Epoch 98/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7851.5255 - val_loss: 8162.5885\n",
      "Epoch 99/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7159.8378 - val_loss: 8372.2134\n",
      "Epoch 100/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 7189.2947 - val_loss: 11078.5820\n"
     ]
    }
   ],
   "source": [
    "model_cplx = build_mode([128, 64], 17) # 1 hidden layer\n",
    "\n",
    "model_cplx.compile(optimizer=tf.keras.optimizers.Adam(lr=0.008), loss='mse')\n",
    "model_cplx.summary()\n",
    "history = model_cplx.fit(x_scale_temp, y_training, epochs=100, batch_size=128, validation_data=(x_val_temp, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss ~ 7189 overfitting after epoch 75. add callback to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 128)               2304      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 10,625\n",
      "Trainable params: 10,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8708 samples, validate on 2178 samples\n",
      "Epoch 1/100\n",
      "8708/8708 [==============================] - 0s 29us/sample - loss: 30764.9489 - val_loss: 22591.3319\n",
      "Epoch 2/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 22348.9321 - val_loss: 21691.6663\n",
      "Epoch 3/100\n",
      "8708/8708 [==============================] - 0s 14us/sample - loss: 20679.7839 - val_loss: 23753.8158\n",
      "Epoch 4/100\n",
      "8708/8708 [==============================] - 0s 19us/sample - loss: 20030.6536 - val_loss: 18810.3811\n",
      "Epoch 5/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 18466.3120 - val_loss: 16794.2479\n",
      "Epoch 6/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 17195.1869 - val_loss: 16228.8937\n",
      "Epoch 7/100\n",
      "8708/8708 [==============================] - 0s 16us/sample - loss: 16574.4240 - val_loss: 15431.9700\n",
      "Epoch 8/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 16075.9318 - val_loss: 15525.0764\n",
      "Epoch 9/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 15708.7587 - val_loss: 14878.0427\n",
      "Epoch 10/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 15169.2795 - val_loss: 14800.2742\n",
      "Epoch 11/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 14954.6211 - val_loss: 13900.9835\n",
      "Epoch 12/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 14690.1659 - val_loss: 16921.2349\n",
      "Epoch 13/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 14897.4233 - val_loss: 13548.2900\n",
      "Epoch 14/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 13969.6613 - val_loss: 16072.2220\n",
      "Epoch 15/100\n",
      "8708/8708 [==============================] - 0s 14us/sample - loss: 15357.4252 - val_loss: 13811.8184\n",
      "Epoch 16/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 13969.7809 - val_loss: 12706.7987\n",
      "Epoch 17/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 13245.9989 - val_loss: 13836.0044\n",
      "Epoch 18/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 13230.1767 - val_loss: 12819.5278\n",
      "Epoch 19/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 13551.2864 - val_loss: 13087.4183\n",
      "Epoch 20/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 13175.7599 - val_loss: 12249.6557\n",
      "Epoch 21/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 12572.9100 - val_loss: 12076.7764\n",
      "Epoch 22/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 12128.7775 - val_loss: 11895.5655\n",
      "Epoch 23/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 12372.9587 - val_loss: 11269.2509\n",
      "Epoch 24/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 11769.6177 - val_loss: 11184.8635\n",
      "Epoch 25/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 11583.8727 - val_loss: 11123.7483\n",
      "Epoch 26/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 11403.8039 - val_loss: 12081.2647\n",
      "Epoch 27/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 11207.3705 - val_loss: 16563.5634\n",
      "Epoch 28/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 12622.1230 - val_loss: 11702.9262\n",
      "Epoch 29/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 11313.7755 - val_loss: 10440.9492\n",
      "Epoch 30/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 10755.4817 - val_loss: 10474.6540\n",
      "Epoch 31/100\n",
      "8708/8708 [==============================] - 0s 18us/sample - loss: 10786.2477 - val_loss: 10441.9526\n",
      "Epoch 32/100\n",
      "8708/8708 [==============================] - 0s 16us/sample - loss: 10586.8927 - val_loss: 10413.9411\n",
      "Epoch 33/100\n",
      "8708/8708 [==============================] - 0s 16us/sample - loss: 10242.2613 - val_loss: 14000.7979\n",
      "Epoch 34/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 11393.2658 - val_loss: 9732.0272\n",
      "Epoch 35/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 11094.8613 - val_loss: 11819.4053\n",
      "Epoch 36/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 10886.8599 - val_loss: 10398.9915\n",
      "Epoch 37/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 10326.1174 - val_loss: 10387.3518\n",
      "Epoch 38/100\n",
      "8708/8708 [==============================] - 0s 14us/sample - loss: 10084.9068 - val_loss: 11980.8793\n",
      "Epoch 39/100\n",
      "8708/8708 [==============================] - 0s 14us/sample - loss: 10917.1510 - val_loss: 9490.7040\n",
      "Epoch 40/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 9241.7261 - val_loss: 9050.9272\n",
      "Epoch 41/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 8983.8320 - val_loss: 9262.9130\n",
      "Epoch 42/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 9193.6162 - val_loss: 8874.5730\n",
      "Epoch 43/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 9230.2924 - val_loss: 10354.2450\n",
      "Epoch 44/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 9250.6596 - val_loss: 9753.4200\n",
      "Epoch 45/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 8709.1178 - val_loss: 9771.0394\n",
      "Epoch 46/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 8691.3261 - val_loss: 13963.8580\n",
      "Epoch 47/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 9695.3430 - val_loss: 8855.8858\n",
      "Epoch 48/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 8445.6673 - val_loss: 8499.1140\n",
      "Epoch 49/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 8876.4255 - val_loss: 11532.9321\n",
      "Epoch 50/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 9224.5816 - val_loss: 11012.3163\n",
      "Epoch 51/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 14300.0710 - val_loss: 9561.0579\n",
      "Epoch 52/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 8540.2143 - val_loss: 8019.9109\n",
      "Epoch 53/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 8818.5594 - val_loss: 7832.1784\n",
      "Epoch 54/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7707.0380 - val_loss: 8253.9287\n",
      "Epoch 55/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 8247.6445 - val_loss: 9532.2016\n",
      "Epoch 56/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7690.1251 - val_loss: 8874.1486\n",
      "Epoch 57/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 7686.9171 - val_loss: 7232.5838\n",
      "Epoch 58/100\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 7509.5952 - val_loss: 8498.0244\n",
      "Epoch 59/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 7866.0542 - val_loss: 7671.9982\n",
      "Epoch 60/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 7491.3488 - val_loss: 7680.1160\n",
      "Epoch 61/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7173.2146 - val_loss: 8176.6752\n",
      "Epoch 62/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7499.8994 - val_loss: 10444.1194\n",
      "Epoch 63/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 8882.1217 - val_loss: 7563.7304\n",
      "Epoch 64/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7142.2082 - val_loss: 8241.0394\n",
      "Epoch 65/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 7805.8226 - val_loss: 6954.6776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 7225.5794 - val_loss: 8644.4055\n",
      "Epoch 67/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7180.5118 - val_loss: 7331.6716\n",
      "Epoch 68/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6874.2801 - val_loss: 7533.6175\n",
      "Epoch 69/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7095.3410 - val_loss: 7977.9810\n",
      "Epoch 70/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6933.8526 - val_loss: 9094.2130\n",
      "Epoch 71/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 6783.4503 - val_loss: 7488.7709\n",
      "Epoch 72/100\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 6515.5677 - val_loss: 7347.4599\n",
      "Epoch 73/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 6905.2987 - val_loss: 16452.7931\n",
      "Epoch 74/100\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 14916.7052 - val_loss: 10730.9575\n",
      "Epoch 75/100\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 10040.9487 - val_loss: 9900.5699\n",
      "Epoch 00075: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=1, patience=10, verbose=1, mode='min')\n",
    "model_cplx_cb = build_mode([128, 64], 17) # 1 hidden layer\n",
    "\n",
    "model_cplx_cb.compile(optimizer=tf.keras.optimizers.Adam(lr=0.008), loss='mse')\n",
    "model_cplx_cb.summary()\n",
    "history = model_cplx_cb.fit(x_scale_temp, y_training, epochs=100, batch_size=128, validation_data=(x_val_temp, y_validation), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss ~ 9900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 128)               2304      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 10,625\n",
      "Trainable params: 10,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8708 samples, validate on 2178 samples\n",
      "Epoch 1/100\n",
      "6528/8708 [=====================>........] - ETA: 0s - loss: 34712.0546\n",
      "Epoch 00001: val_loss improved from inf to 22925.00760, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 31us/sample - loss: 32169.4382 - val_loss: 22925.0076\n",
      "Epoch 2/100\n",
      "5760/8708 [==================>...........] - ETA: 0s - loss: 22296.0347\n",
      "Epoch 00002: val_loss did not improve from 22925.00760\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 22722.4092 - val_loss: 24387.5974\n",
      "Epoch 3/100\n",
      "5888/8708 [===================>..........] - ETA: 0s - loss: 21823.9920\n",
      "Epoch 00003: val_loss improved from 22925.00760 to 21069.64333, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 21778.8941 - val_loss: 21069.6433\n",
      "Epoch 4/100\n",
      "5504/8708 [=================>............] - ETA: 0s - loss: 21293.0671\n",
      "Epoch 00004: val_loss improved from 21069.64333 to 18927.96380, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 20718.5748 - val_loss: 18927.9638\n",
      "Epoch 5/100\n",
      "5376/8708 [=================>............] - ETA: 0s - loss: 19727.0742\n",
      "Epoch 00005: val_loss improved from 18927.96380 to 18342.86268, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 14us/sample - loss: 19572.7266 - val_loss: 18342.8627\n",
      "Epoch 6/100\n",
      "5376/8708 [=================>............] - ETA: 0s - loss: 18583.3181\n",
      "Epoch 00006: val_loss did not improve from 18342.86268\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 18296.7036 - val_loss: 19074.4512\n",
      "Epoch 7/100\n",
      "5504/8708 [=================>............] - ETA: 0s - loss: 17710.3423\n",
      "Epoch 00007: val_loss did not improve from 18342.86268\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 17449.4228 - val_loss: 20519.9278\n",
      "Epoch 8/100\n",
      "5504/8708 [=================>............] - ETA: 0s - loss: 17274.5607\n",
      "Epoch 00008: val_loss did not improve from 18342.86268\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 17007.5124 - val_loss: 19133.4947\n",
      "Epoch 9/100\n",
      "4480/8708 [==============>...............] - ETA: 0s - loss: 17856.6849\n",
      "Epoch 00009: val_loss improved from 18342.86268 to 15886.46324, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 15us/sample - loss: 16746.0493 - val_loss: 15886.4632\n",
      "Epoch 10/100\n",
      "5504/8708 [=================>............] - ETA: 0s - loss: 15920.7195\n",
      "Epoch 00010: val_loss did not improve from 15886.46324\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 15922.0401 - val_loss: 16265.9052\n",
      "Epoch 11/100\n",
      "5248/8708 [=================>............] - ETA: 0s - loss: 15457.9645\n",
      "Epoch 00011: val_loss improved from 15886.46324 to 14458.54625, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 14us/sample - loss: 15347.2940 - val_loss: 14458.5463\n",
      "Epoch 12/100\n",
      "5760/8708 [==================>...........] - ETA: 0s - loss: 14762.7807\n",
      "Epoch 00012: val_loss improved from 14458.54625 to 14005.65319, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 14710.3023 - val_loss: 14005.6532\n",
      "Epoch 13/100\n",
      "5376/8708 [=================>............] - ETA: 0s - loss: 14198.3626\n",
      "Epoch 00013: val_loss did not improve from 14005.65319\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 14529.4410 - val_loss: 14660.7573\n",
      "Epoch 14/100\n",
      "4608/8708 [==============>...............] - ETA: 0s - loss: 14094.5329\n",
      "Epoch 00014: val_loss improved from 14005.65319 to 13628.22955, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 16us/sample - loss: 14170.1632 - val_loss: 13628.2296\n",
      "Epoch 15/100\n",
      "4864/8708 [===============>..............] - ETA: 0s - loss: 13876.3290\n",
      "Epoch 00015: val_loss improved from 13628.22955 to 13131.51115, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 14us/sample - loss: 13498.2090 - val_loss: 13131.5111\n",
      "Epoch 16/100\n",
      "5376/8708 [=================>............] - ETA: 0s - loss: 14945.2329\n",
      "Epoch 00016: val_loss improved from 13131.51115 to 13111.08780, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 14us/sample - loss: 14543.8104 - val_loss: 13111.0878\n",
      "Epoch 17/100\n",
      "5760/8708 [==================>...........] - ETA: 0s - loss: 12704.6254\n",
      "Epoch 00017: val_loss did not improve from 13111.08780\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 13028.9585 - val_loss: 17694.9367\n",
      "Epoch 18/100\n",
      "6016/8708 [===================>..........] - ETA: 0s - loss: 14106.5867\n",
      "Epoch 00018: val_loss improved from 13111.08780 to 12013.59746, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 13763.4413 - val_loss: 12013.5975\n",
      "Epoch 19/100\n",
      "4480/8708 [==============>...............] - ETA: 0s - loss: 13297.1102\n",
      "Epoch 00019: val_loss improved from 12013.59746 to 11655.34239, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 14us/sample - loss: 12758.5333 - val_loss: 11655.3424\n",
      "Epoch 20/100\n",
      "5632/8708 [==================>...........] - ETA: 0s - loss: 11866.1311\n",
      "Epoch 00020: val_loss did not improve from 11655.34239\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 12186.5431 - val_loss: 14337.6587\n",
      "Epoch 21/100\n",
      "5504/8708 [=================>............] - ETA: 0s - loss: 12560.7258\n",
      "Epoch 00021: val_loss improved from 11655.34239 to 11182.71597, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 12445.4367 - val_loss: 11182.7160\n",
      "Epoch 22/100\n",
      "5760/8708 [==================>...........] - ETA: 0s - loss: 11561.3659\n",
      "Epoch 00022: val_loss did not improve from 11182.71597\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 11670.8724 - val_loss: 12216.7346\n",
      "Epoch 23/100\n",
      "5248/8708 [=================>............] - ETA: 0s - loss: 11582.8388\n",
      "Epoch 00023: val_loss did not improve from 11182.71597\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 11236.2991 - val_loss: 13289.7037\n",
      "Epoch 24/100\n",
      "5760/8708 [==================>...........] - ETA: 0s - loss: 11994.3633\n",
      "Epoch 00024: val_loss improved from 11182.71597 to 10677.57662, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 11568.5035 - val_loss: 10677.5766\n",
      "Epoch 25/100\n",
      "5248/8708 [=================>............] - ETA: 0s - loss: 10832.2388\n",
      "Epoch 00025: val_loss did not improve from 10677.57662\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 10789.7801 - val_loss: 12702.1280\n",
      "Epoch 26/100\n",
      "6272/8708 [====================>.........] - ETA: 0s - loss: 10607.7334\n",
      "Epoch 00026: val_loss did not improve from 10677.57662\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 10876.4330 - val_loss: 22008.9112\n",
      "Epoch 27/100\n",
      "5760/8708 [==================>...........] - ETA: 0s - loss: 15097.9001\n",
      "Epoch 00027: val_loss improved from 10677.57662 to 10238.42718, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 13649.8517 - val_loss: 10238.4272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "5888/8708 [===================>..........] - ETA: 0s - loss: 10256.8066\n",
      "Epoch 00028: val_loss did not improve from 10238.42718\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 10080.6865 - val_loss: 10790.0907\n",
      "Epoch 29/100\n",
      "6528/8708 [=====================>........] - ETA: 0s - loss: 9845.9457\n",
      "Epoch 00029: val_loss did not improve from 10238.42718\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 10076.2375 - val_loss: 11057.2372\n",
      "Epoch 30/100\n",
      "7168/8708 [=======================>......] - ETA: 0s - loss: 10102.3228\n",
      "Epoch 00030: val_loss did not improve from 10238.42718\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 10098.1030 - val_loss: 11828.0380\n",
      "Epoch 31/100\n",
      "6144/8708 [====================>.........] - ETA: 0s - loss: 10391.2796\n",
      "Epoch 00031: val_loss did not improve from 10238.42718\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 10087.8390 - val_loss: 10756.8650\n",
      "Epoch 32/100\n",
      "6016/8708 [===================>..........] - ETA: 0s - loss: 10039.7899\n",
      "Epoch 00032: val_loss improved from 10238.42718 to 9041.75556, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 9958.0160 - val_loss: 9041.7556\n",
      "Epoch 33/100\n",
      "6272/8708 [====================>.........] - ETA: 0s - loss: 9964.0989\n",
      "Epoch 00033: val_loss did not improve from 9041.75556\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 9731.8564 - val_loss: 9672.2338\n",
      "Epoch 34/100\n",
      "5376/8708 [=================>............] - ETA: 0s - loss: 10052.8460\n",
      "Epoch 00034: val_loss did not improve from 9041.75556\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 10008.1003 - val_loss: 10415.7752\n",
      "Epoch 35/100\n",
      "6016/8708 [===================>..........] - ETA: 0s - loss: 10620.7618\n",
      "Epoch 00035: val_loss did not improve from 9041.75556\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 10410.4443 - val_loss: 9803.8590\n",
      "Epoch 36/100\n",
      "7552/8708 [=========================>....] - ETA: 0s - loss: 9852.9407\n",
      "Epoch 00036: val_loss did not improve from 9041.75556\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 9797.4464 - val_loss: 9578.5941\n",
      "Epoch 37/100\n",
      "6656/8708 [=====================>........] - ETA: 0s - loss: 10646.8058\n",
      "Epoch 00037: val_loss did not improve from 9041.75556\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 10505.9937 - val_loss: 10228.1737\n",
      "Epoch 38/100\n",
      "5504/8708 [=================>............] - ETA: 0s - loss: 9359.4258 \n",
      "Epoch 00038: val_loss did not improve from 9041.75556\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 9356.0340 - val_loss: 9168.6515\n",
      "Epoch 39/100\n",
      "4992/8708 [================>.............] - ETA: 0s - loss: 9868.4556\n",
      "Epoch 00039: val_loss improved from 9041.75556 to 8688.68391, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 14us/sample - loss: 9448.8481 - val_loss: 8688.6839\n",
      "Epoch 40/100\n",
      "5632/8708 [==================>...........] - ETA: 0s - loss: 8779.1096\n",
      "Epoch 00040: val_loss improved from 8688.68391 to 8444.07714, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 8763.6278 - val_loss: 8444.0771\n",
      "Epoch 41/100\n",
      "5632/8708 [==================>...........] - ETA: 0s - loss: 9275.2349\n",
      "Epoch 00041: val_loss did not improve from 8444.07714\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 8948.6255 - val_loss: 8580.6566\n",
      "Epoch 42/100\n",
      "5760/8708 [==================>...........] - ETA: 0s - loss: 8080.0023\n",
      "Epoch 00042: val_loss improved from 8444.07714 to 8256.60654, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 8516.5865 - val_loss: 8256.6065\n",
      "Epoch 43/100\n",
      "5504/8708 [=================>............] - ETA: 0s - loss: 9007.5267\n",
      "Epoch 00043: val_loss did not improve from 8256.60654\n",
      "8708/8708 [==============================] - 0s 12us/sample - loss: 8882.4591 - val_loss: 10163.1285\n",
      "Epoch 44/100\n",
      "5632/8708 [==================>...........] - ETA: 0s - loss: 8029.5446\n",
      "Epoch 00044: val_loss did not improve from 8256.60654\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 8330.4806 - val_loss: 9042.0480\n",
      "Epoch 45/100\n",
      "6656/8708 [=====================>........] - ETA: 0s - loss: 8700.4069\n",
      "Epoch 00045: val_loss did not improve from 8256.60654\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 8651.3311 - val_loss: 10685.9197\n",
      "Epoch 46/100\n",
      "6912/8708 [======================>.......] - ETA: 0s - loss: 9595.0635\n",
      "Epoch 00046: val_loss improved from 8256.60654 to 8225.64678, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 9290.9649 - val_loss: 8225.6468\n",
      "Epoch 47/100\n",
      "6528/8708 [=====================>........] - ETA: 0s - loss: 7775.4526\n",
      "Epoch 00047: val_loss did not improve from 8225.64678\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 7874.3890 - val_loss: 9168.2880\n",
      "Epoch 48/100\n",
      "6272/8708 [====================>.........] - ETA: 0s - loss: 9494.2125\n",
      "Epoch 00048: val_loss did not improve from 8225.64678\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 9107.2954 - val_loss: 9194.9423\n",
      "Epoch 49/100\n",
      "6016/8708 [===================>..........] - ETA: 0s - loss: 9014.0479\n",
      "Epoch 00049: val_loss did not improve from 8225.64678\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 8570.7094 - val_loss: 10242.8752\n",
      "Epoch 50/100\n",
      "5504/8708 [=================>............] - ETA: 0s - loss: 7761.0869\n",
      "Epoch 00050: val_loss improved from 8225.64678 to 7596.19874, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 13us/sample - loss: 7958.1825 - val_loss: 7596.1987\n",
      "Epoch 51/100\n",
      "6272/8708 [====================>.........] - ETA: 0s - loss: 7949.5505 \n",
      "Epoch 00051: val_loss improved from 7596.19874 to 7445.67843, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7750.3790 - val_loss: 7445.6784\n",
      "Epoch 52/100\n",
      "7296/8708 [========================>.....] - ETA: 0s - loss: 7582.3690\n",
      "Epoch 00052: val_loss did not improve from 7445.67843\n",
      "8708/8708 [==============================] - 0s 8us/sample - loss: 7606.1392 - val_loss: 8156.5239\n",
      "Epoch 53/100\n",
      "6016/8708 [===================>..........] - ETA: 0s - loss: 7424.6451\n",
      "Epoch 00053: val_loss improved from 7445.67843 to 7351.98142, saving model to /tmp/kaggle-bike/models/model.hdf5\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7546.8951 - val_loss: 7351.9814\n",
      "Epoch 54/100\n",
      "7424/8708 [========================>.....] - ETA: 0s - loss: 7284.1759\n",
      "Epoch 00054: val_loss did not improve from 7351.98142\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 7386.3075 - val_loss: 8892.2300\n",
      "Epoch 55/100\n",
      "5760/8708 [==================>...........] - ETA: 0s - loss: 8371.7657\n",
      "Epoch 00055: val_loss did not improve from 7351.98142\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 8089.6358 - val_loss: 7502.3383\n",
      "Epoch 56/100\n",
      "6912/8708 [======================>.......] - ETA: 0s - loss: 7624.9898\n",
      "Epoch 00056: val_loss did not improve from 7351.98142\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 8087.4027 - val_loss: 11650.4619\n",
      "Epoch 57/100\n",
      "6400/8708 [=====================>........] - ETA: 0s - loss: 10256.6921\n",
      "Epoch 00057: val_loss did not improve from 7351.98142\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 9745.7561 - val_loss: 7850.7841\n",
      "Epoch 58/100\n",
      "5760/8708 [==================>...........] - ETA: 0s - loss: 7919.9655\n",
      "Epoch 00058: val_loss did not improve from 7351.98142\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7833.7783 - val_loss: 21075.3293\n",
      "Epoch 59/100\n",
      "5888/8708 [===================>..........] - ETA: 0s - loss: 11161.5135\n",
      "Epoch 00059: val_loss did not improve from 7351.98142\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 10012.1657 - val_loss: 8109.6927\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5760/8708 [==================>...........] - ETA: 0s - loss: 7031.3567\n",
      "Epoch 00060: val_loss did not improve from 7351.98142\n",
      "8708/8708 [==============================] - 0s 11us/sample - loss: 7117.2973 - val_loss: 7424.7381\n",
      "Epoch 61/100\n",
      "6656/8708 [=====================>........] - ETA: 0s - loss: 7227.2996\n",
      "Epoch 00061: val_loss did not improve from 7351.98142\n",
      "8708/8708 [==============================] - 0s 9us/sample - loss: 7222.3915 - val_loss: 7572.3421\n",
      "Epoch 62/100\n",
      "5760/8708 [==================>...........] - ETA: 0s - loss: 6937.1318\n",
      "Epoch 00062: val_loss did not improve from 7351.98142\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 7012.2667 - val_loss: 8657.5528\n",
      "Epoch 63/100\n",
      "6144/8708 [====================>.........] - ETA: 0s - loss: 7077.6342\n",
      "Epoch 00063: val_loss did not improve from 7351.98142\n",
      "8708/8708 [==============================] - 0s 10us/sample - loss: 7013.9157 - val_loss: 16336.1205\n",
      "Epoch 00063: early stopping\n"
     ]
    }
   ],
   "source": [
    "check_point = ModelCheckpoint('/tmp/kaggle-bike/models/model.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=1, patience=10, verbose=1, mode='min')\n",
    "model_cplx_cb_sv = build_mode([128, 64], 17) # 1 hidden layer\n",
    "\n",
    "model_cplx_cb_sv.compile(optimizer=tf.keras.optimizers.Adam(lr=0.008), loss='mse')\n",
    "model_cplx_cb_sv.summary()\n",
    "history = model_cplx_cb_sv.fit(x_scale_temp, y_training, epochs=100, batch_size=128, validation_data=(x_val_temp, y_validation), callbacks=[early_stop, check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
